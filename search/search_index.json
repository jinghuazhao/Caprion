{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Protein analysis workflow (experimental) module add ceuadmin/snakemake snakemake -s workflow/rules/cojo.smk -j1 snakemake -s workflow/rules/report.smk -j1 snakemake -s workflow/rules/cojo.smk -c --profile workflow and use --unlock when necessary. Programs 1 Earlier work was done in a named sequence. 1_pca_projection.sh 2_ggm.R 3_wgcna.R 4_pca_clustering.R 5_pgwas.sh 6_meta_analysis.sh 7_merge.sh 8_hla.sh 9_lookup.sh 1. Data handling and PCA projection The pipeline follows HGI contributions nevertheless only serves for reassurance since the study samples were carefully selected. 2. GGM The results are ready to report. 3. WGCNA This can be finalised according to the Science paper. 4. PCA and clustering The groupings based on unfiltered and DR-filtered proteins can be made on three phases altogether and instead of a classification indicator the first three PCs are used. The PLINK2 has been consistent in the pilot studies, so scale() operaions can be used in the inverse normal transformations. The phenotypic data is generated in accordance with the double transformations as in SCALLOP-Seq analysis. 5. pGWAS 2 The bgen files were extracted from a list of all samples, the variant IDs of which were for all RSids to allow for multiallelic loci. The bgen generation is moved into .sb based on cclake but can be switched back to cardio by uncommenting the ##SBATCH lines. The (sb)atch file is extended to produce Q-Q/Manhattan/LocusZoom plots and extreme p values are possible for all plots. Note that LocusZoom 1.4 does not contain 1000Genomes build 37 genotypes for chromosome X and therefore they are supplemented with local files in the required format, namely, locuszoom_1.4/data/1000G/genotypes/2014-10-14/EUR/chrX.[bed, bim, fan] . 6. Meta-analysis This follows from the SCALLOP/INF implementation, as designed analogous to a Makefile, i.e., 6_meta_analysis <task> where task = METAL_list, METAL_files, METAL_analysis, respectively in sequence. To extract significant variants one may resort to awk 'NR==1||$12<log(1e-6)/log(10)' 1433B-1.tbl , say. 7. Variant identification An iterative merging scheme is employed; the HLA region is simplified but will be specifically handled. Somewhat paradoxically, forest plots are also obtained here 3 . 8. HLA imputation 4 This is experimented on several software including HIBAG, CookHLA and SNP2HLA as desribed here . The whole cohort imputation requests resources exceeding the system limits, so a cardio SLURM job is used instead. The hped file from CookHLA (or converted from HIBAG) can be used by HATK for association analysis while the advantage of SNP2HLA is that binary ped files are ready for use as usual. 9. Lookup Directories This is per Caprion project bash module load miniconda3/4.5.1 export csd3path=/rds/project/jmmh2/rds-jmmh2-projects/olink_proteomics/scallop/miniconda37 source ${csd3path}/bin/activate Name Description pgwas pGWAS METAL Meta-analysis HLA HLA imputation peptide_progs peptide analysis reports Reports Note that docs.sh copies pilot/utils directory of the pilot studies, so coding under that directory is preferable to avoid overwrite. To accommodate filteredd results, a suffix \"\" or \"_dr\" is applied when appropriate. \u21a9 Protein GWAS GCTA/fastGWA employs MAF>=0.001 (~56%) and geno=0.1 so potentially we can have .bgen files as such to speed up. GCTA uses headerless phenotype files, so the following section from 5_pgwas.sh is run in preparation. bash sed -i '1d' ${caprion}/work/caprion-1.pheno sed -i '1d' ${caprion}/work/caprion-2.pheno sed -i '1d' ${caprion}/work/caprion-3.pheno at pilot/work while the original version is saved at analysis/work/ . It looked to take 3.5 days on Cardio without unfiltered genotypes but ~12 hours on cclake, and once these are taken care of the analysis can be propagated. \u21a9 incomplete gamma function The .info files for proteins BROX and CT027 could not be obtained from METAL 2020-05-05 with the following error message, FATAL ERROR - a too large, ITMAX too small in gamma countinued fraction (gcf) An attempt was made to fix this and reported as a fixable issue to METAL GitHub respository ( https://github.com/statgen/METAL/issues/24 ). This has enabled Forest plots for the associate pQTLs. \u21a9 Whole cohort imputation is feasible with a HIBAG reference panel, Locus A B C DPB1 DQA1 DQB1 DRB1 N 1857 2572 1866 1624 1740 1924 2436 SNPs 891 990 1041 689 948 979 891 while the reference panel is based on the 1000Genomes data (N=503) with SNP2HLA and CookHLA. It is of note that 1000G_REF.EUR.chr6.hg18.29mb-34mb.inT1DGC.markers in the 1000Genomes reference panel has 465 variants with HLA prefix and the partition is as follows, Locus A B C DPB1 DQA1 DQB1 DRB1 HLA_ 98 183 69 0 0 33 82 A recent update: PGG.HLA, https://pog.fudan.edu.cn/pggmhc/ , requires data submission. \u21a9","title":"Index"},{"location":"#protein-analysis","text":"","title":"Protein analysis"},{"location":"#workflow-experimental","text":"module add ceuadmin/snakemake snakemake -s workflow/rules/cojo.smk -j1 snakemake -s workflow/rules/report.smk -j1 snakemake -s workflow/rules/cojo.smk -c --profile workflow and use --unlock when necessary.","title":"workflow (experimental)"},{"location":"#programs1","text":"Earlier work was done in a named sequence. 1_pca_projection.sh 2_ggm.R 3_wgcna.R 4_pca_clustering.R 5_pgwas.sh 6_meta_analysis.sh 7_merge.sh 8_hla.sh 9_lookup.sh","title":"Programs1"},{"location":"#1-data-handling-and-pca-projection","text":"The pipeline follows HGI contributions nevertheless only serves for reassurance since the study samples were carefully selected.","title":"1. Data handling and PCA projection"},{"location":"#2-ggm","text":"The results are ready to report.","title":"2. GGM"},{"location":"#3-wgcna","text":"This can be finalised according to the Science paper.","title":"3. WGCNA"},{"location":"#4-pca-and-clustering","text":"The groupings based on unfiltered and DR-filtered proteins can be made on three phases altogether and instead of a classification indicator the first three PCs are used. The PLINK2 has been consistent in the pilot studies, so scale() operaions can be used in the inverse normal transformations. The phenotypic data is generated in accordance with the double transformations as in SCALLOP-Seq analysis.","title":"4. PCA and clustering"},{"location":"#5-pgwas2","text":"The bgen files were extracted from a list of all samples, the variant IDs of which were for all RSids to allow for multiallelic loci. The bgen generation is moved into .sb based on cclake but can be switched back to cardio by uncommenting the ##SBATCH lines. The (sb)atch file is extended to produce Q-Q/Manhattan/LocusZoom plots and extreme p values are possible for all plots. Note that LocusZoom 1.4 does not contain 1000Genomes build 37 genotypes for chromosome X and therefore they are supplemented with local files in the required format, namely, locuszoom_1.4/data/1000G/genotypes/2014-10-14/EUR/chrX.[bed, bim, fan] .","title":"5. pGWAS2"},{"location":"#6-meta-analysis","text":"This follows from the SCALLOP/INF implementation, as designed analogous to a Makefile, i.e., 6_meta_analysis <task> where task = METAL_list, METAL_files, METAL_analysis, respectively in sequence. To extract significant variants one may resort to awk 'NR==1||$12<log(1e-6)/log(10)' 1433B-1.tbl , say.","title":"6. Meta-analysis"},{"location":"#7-variant-identification","text":"An iterative merging scheme is employed; the HLA region is simplified but will be specifically handled. Somewhat paradoxically, forest plots are also obtained here 3 .","title":"7. Variant identification"},{"location":"#8-hla-imputation4","text":"This is experimented on several software including HIBAG, CookHLA and SNP2HLA as desribed here . The whole cohort imputation requests resources exceeding the system limits, so a cardio SLURM job is used instead. The hped file from CookHLA (or converted from HIBAG) can be used by HATK for association analysis while the advantage of SNP2HLA is that binary ped files are ready for use as usual.","title":"8. HLA imputation4"},{"location":"#9-lookup","text":"Directories This is per Caprion project bash module load miniconda3/4.5.1 export csd3path=/rds/project/jmmh2/rds-jmmh2-projects/olink_proteomics/scallop/miniconda37 source ${csd3path}/bin/activate Name Description pgwas pGWAS METAL Meta-analysis HLA HLA imputation peptide_progs peptide analysis reports Reports Note that docs.sh copies pilot/utils directory of the pilot studies, so coding under that directory is preferable to avoid overwrite. To accommodate filteredd results, a suffix \"\" or \"_dr\" is applied when appropriate. \u21a9 Protein GWAS GCTA/fastGWA employs MAF>=0.001 (~56%) and geno=0.1 so potentially we can have .bgen files as such to speed up. GCTA uses headerless phenotype files, so the following section from 5_pgwas.sh is run in preparation. bash sed -i '1d' ${caprion}/work/caprion-1.pheno sed -i '1d' ${caprion}/work/caprion-2.pheno sed -i '1d' ${caprion}/work/caprion-3.pheno at pilot/work while the original version is saved at analysis/work/ . It looked to take 3.5 days on Cardio without unfiltered genotypes but ~12 hours on cclake, and once these are taken care of the analysis can be propagated. \u21a9 incomplete gamma function The .info files for proteins BROX and CT027 could not be obtained from METAL 2020-05-05 with the following error message, FATAL ERROR - a too large, ITMAX too small in gamma countinued fraction (gcf) An attempt was made to fix this and reported as a fixable issue to METAL GitHub respository ( https://github.com/statgen/METAL/issues/24 ). This has enabled Forest plots for the associate pQTLs. \u21a9 Whole cohort imputation is feasible with a HIBAG reference panel, Locus A B C DPB1 DQA1 DQB1 DRB1 N 1857 2572 1866 1624 1740 1924 2436 SNPs 891 990 1041 689 948 979 891 while the reference panel is based on the 1000Genomes data (N=503) with SNP2HLA and CookHLA. It is of note that 1000G_REF.EUR.chr6.hg18.29mb-34mb.inT1DGC.markers in the 1000Genomes reference panel has 465 variants with HLA prefix and the partition is as follows, Locus A B C DPB1 DQA1 DQB1 DRB1 HLA_ 98 183 69 0 0 33 82 A recent update: PGG.HLA, https://pog.fudan.edu.cn/pggmhc/ , requires data submission. \u21a9","title":"9. Lookup"},{"location":"peptide_progs/","text":"Peptide analysis Peptide-level analysis This mirrors protein-level analysis as in peptide_progs/, 0_utils.sh 1_pgwas.sh 2_meta_analysis.sh 3_merge.sh A prerequiste for a Manhattan/peptide association plot by 0_utils.sh is a call to gz() for a compressed DR-filtered data; this is to be followed by execution of the R script inside. with respect to Association analysis. Meta-analysis. Signal detection/classification, forest, Q-Q, Manhattan, LocusZoom, mean-by-genotype/dosage plots. NB. 3_merge.sh itself is comprised of three steps on signal extraction, collection, and graphical representation which is operationally done through commenting/uncommenting calls to each step exclusively. In particular, CO3 and ITIH2 are resumed after the 12hr threshold for SLURM. Both 0_utils.sh and the second step 3.2 above both requires Ensembl-VEP but ceuadmin/ensembl-vep/104 does not work on icelake; a remedy is made with ceuadmin/ensembl-vep/111-icelake however the loftee plugin is likely to require additional work -- as documented this is feasible with vep -i variants.vcf --plugin LoFtool,scores_file.txt .","title":"Peptide analysis"},{"location":"peptide_progs/#peptide-analysis","text":"Peptide-level analysis This mirrors protein-level analysis as in peptide_progs/, 0_utils.sh 1_pgwas.sh 2_meta_analysis.sh 3_merge.sh A prerequiste for a Manhattan/peptide association plot by 0_utils.sh is a call to gz() for a compressed DR-filtered data; this is to be followed by execution of the R script inside. with respect to Association analysis. Meta-analysis. Signal detection/classification, forest, Q-Q, Manhattan, LocusZoom, mean-by-genotype/dosage plots. NB. 3_merge.sh itself is comprised of three steps on signal extraction, collection, and graphical representation which is operationally done through commenting/uncommenting calls to each step exclusively. In particular, CO3 and ITIH2 are resumed after the 12hr threshold for SLURM. Both 0_utils.sh and the second step 3.2 above both requires Ensembl-VEP but ceuadmin/ensembl-vep/104 does not work on icelake; a remedy is made with ceuadmin/ensembl-vep/111-icelake however the loftee plugin is likely to require additional work -- as documented this is feasible with vep -i variants.vcf --plugin LoFtool,scores_file.txt .","title":"Peptide analysis"},{"location":"pilot/","text":"Pilot studies Site map Pilot (N=196) data/ contains genotype files in .bgen format bgen/ PLINK2 results according to .bgen files; summary outputs and sentinels/ directory are in the following directories 1e-5 1e-6 5e-8 Batch 2 (N=1,488) data2/ contains genotype files in .bgen format bgen2/ PLINK2 results according to .bgen files; summary outputs and sentinels/ directory are in the following directories 1e-5 5e-8 Comparison of pilot and batch 2 miamiplot Batch 3 data (N=807) data3/ .bgen data bgen3/ PLINK2 results 1e-5 5e-8 Coding There are apparent commonalities between batches from the list of programs and diagrams; many of which are activated as subroutines. Pilot caprion.R and caprion.ini are for data processing. Their derivatives are in the utils/ subdirectory: affymetrix.sh is for variant-specific association analysis. qctool.sb is used to extract available sample and genotypes. qctool.sh further extracts genotypes with MAF 0.01 only. plink2.sh non-SLURM version of association analysis. qqman.sh and qqman.R produce QQ and Manhattan plots. sentinels_nold.sh and merge.sh select sentinels. ps.sh and ps.R run through PhenoScanner. lookup.sh looks up for overlap with SomaLogic and Olink. caprion.ipynb is a Jupyter notebook with some preprocessing done by tensorqtl.sh . Batch 2 (prefix=utils/ when unspecified) graph TB tensoqtl.sh 2020.sh --> EPCR-PROC/ 2020.sh --> data2/affymetrix.id qctool.sb --> qctool.sh qctool.sh --> plink2.sh plink2.sh --> sentinels_nold.sh sentinels_nold.sh --> merge.sh Batch 3 (prefix=utils/) graph TB 2021.sh 2021.sh --> eSet.R 2021.sh --> 2021.R eSet.R --> 2021.R eSet.R --> UDP.R 2021.sh --> UDP.R UDP.R --> qctool.sb qctool.sb --> qctool.sh qctool.sh --> plink2.* 2021.sh --> plink2.* plink2.* --> sentinels_nold.sh+merge.sh Note that eSet.R actually covers data from pilot, batches 2 and 3. Documents ppr.md EPCR-PROC.md 2021.md Reference Klaus B, Reisenauer S (2018). An end to end workflow for differential gene expression using Affymetrix microarrays . https://bioinformatics.psb.ugent.be/webtools/Venn/","title":"Pilot studies"},{"location":"pilot/#pilot-studies","text":"","title":"Pilot studies"},{"location":"pilot/#site-map","text":"Pilot (N=196) data/ contains genotype files in .bgen format bgen/ PLINK2 results according to .bgen files; summary outputs and sentinels/ directory are in the following directories 1e-5 1e-6 5e-8 Batch 2 (N=1,488) data2/ contains genotype files in .bgen format bgen2/ PLINK2 results according to .bgen files; summary outputs and sentinels/ directory are in the following directories 1e-5 5e-8 Comparison of pilot and batch 2 miamiplot Batch 3 data (N=807) data3/ .bgen data bgen3/ PLINK2 results 1e-5 5e-8","title":"Site map"},{"location":"pilot/#coding","text":"There are apparent commonalities between batches from the list of programs and diagrams; many of which are activated as subroutines. Pilot caprion.R and caprion.ini are for data processing. Their derivatives are in the utils/ subdirectory: affymetrix.sh is for variant-specific association analysis. qctool.sb is used to extract available sample and genotypes. qctool.sh further extracts genotypes with MAF 0.01 only. plink2.sh non-SLURM version of association analysis. qqman.sh and qqman.R produce QQ and Manhattan plots. sentinels_nold.sh and merge.sh select sentinels. ps.sh and ps.R run through PhenoScanner. lookup.sh looks up for overlap with SomaLogic and Olink. caprion.ipynb is a Jupyter notebook with some preprocessing done by tensorqtl.sh . Batch 2 (prefix=utils/ when unspecified) graph TB tensoqtl.sh 2020.sh --> EPCR-PROC/ 2020.sh --> data2/affymetrix.id qctool.sb --> qctool.sh qctool.sh --> plink2.sh plink2.sh --> sentinels_nold.sh sentinels_nold.sh --> merge.sh Batch 3 (prefix=utils/) graph TB 2021.sh 2021.sh --> eSet.R 2021.sh --> 2021.R eSet.R --> 2021.R eSet.R --> UDP.R 2021.sh --> UDP.R UDP.R --> qctool.sb qctool.sb --> qctool.sh qctool.sh --> plink2.* 2021.sh --> plink2.* plink2.* --> sentinels_nold.sh+merge.sh Note that eSet.R actually covers data from pilot, batches 2 and 3.","title":"Coding"},{"location":"pilot/#documents","text":"ppr.md EPCR-PROC.md 2021.md","title":"Documents"},{"location":"pilot/#reference","text":"Klaus B, Reisenauer S (2018). An end to end workflow for differential gene expression using Affymetrix microarrays . https://bioinformatics.psb.ugent.be/webtools/Venn/","title":"Reference"},{"location":"pilot/Notes/","text":"Notes m/z (ChatGPT) In the context of mass spectrometry, the term \"charge\" refers to the electrical charge of an ion. Mass spectrometry is a technique used to analyze the mass-to-charge ratio (m/z) of ions. Monoisotopic m/z refers to the mass-to-charge ratio of the monoisotopic peak, which is the peak corresponding to the ion containing the most abundant isotope of each element. The Max Isotope Time Centroid is the time at which the intensity-weighted average of the isotope distribution reaches its maximum value. This parameter is important for accurately determining the mass and charge state of an ion. The charge of an ion is a crucial piece of information in mass spectrometry. It is used in the calculation of the mass-to-charge ratio (m/z) of an ion, which is a key parameter in identifying and characterizing molecules. The m/z value is used to determine the mass of an ion, and the charge is needed to convert the raw mass spectrometry data (measured in daltons) into the mass-to-charge ratio. The formula for calculating m/z is: \\[ m/z=\\frac{\\mbox{Mass of the ion}}{\\mbox{Charge of the ion\u200b}} \\] The charge of an ion is essential for interpreting mass spectrometry data, particularly in determining the mass-to-charge ratio (m/z) of ions, which is fundamental for identifying and characterizing molecules in a sample. When the charge is 0 for monoisotopic m/z, it implies a neutral species, and the mass spectrometry analysis might be focusing on the measurement of neutral molecules or particles. In proteomics, researchers often analyze proteins and peptides in their neutral forms. Techniques like matrix-assisted laser desorption/ionization (MALDI) are commonly used for the analysis of intact proteins in their neutral state Peptide and Protein ID using OpenMS tools https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-oms/tutorial.html#peptide-identification PoGo Fast Mapping of Peptides to Genomic Coordinates for Proteogenomic Analyses, https://www.sanger.ac.uk/tool/pogo/ , GitHub, https://github.com/cschlaffner/PoGo . It uses transcript translations and reference gene annotations to identify the genomic loci of peptides and post-translational modifications. Multiple occurrences of peptides in the input data resulting in the same genomic loci will be collapsed as a single occurrence in the output. The input format is a tab delimited file with four columns with file extensions such as .pogo, .txt, and *.tsv. Column Column header Description 1 Sample Name of sample or experiment 2 Peptide Peptide sequence with PSI-MS nodification names in round brackets following the mpdified amino acid, e.g. PEPT(Phopsho)IDE for a phosphorylated threonine 3 PSMs Number of peptide-spectrum matches (PSMs) for the given peptide, including those redundantly identified (peptides can be \u201cseen\u201d more than once in a run) 4 Quant Quantitative value for the given peptide in the given sample An example is established as follows, wget -S ftp://ftp.sanger.ac.uk/pub/teams/17/software/PoGo/PoGo_Testprocedures.zip unzip PoGo_Testprocedures.zip cd PoGo_Testprocedures/Testfiles module load ceuadmin/PoGo for Peptides in Testfile_experimental Testfile_small do PoGo -fasta input/gencode.v25.pc_translations.fa -gtf input/gencode.v25.annotation.gtf -in input/${Peptides}.txt done Output files are also contained in the input/ directory. GENCODE annotation data are available from https://www.gencodegenes.org/human/ and https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/ . The Java GUI, https://github.com/cschlaffner/PoGoGUI , is run as follows, java -jar PoGoGUI-v1.0.0.jar which requires PoGo executable as well. The source is compiled with maven, https://maven.apache.org/ , e.g., module load maven-3.5.0-gcc-5.4.0-3sgaeze mvn install assuming that pom.xml is available, e.g., /usr/local/Cluster-Apps/ceuadmin/PoGo/1.0.0/PoGoGUI/PoGoGUI . Proteoform Analysis ETH Zurich, U Toronto Team Develops Tool for Bottom-Up Proteomics Proteoform Analysis Jul 28, 2021 | Adam Bonislawski NEW YORK \u2013 A team led by researchers at ETH Zurich and the University of Toronto has developed a tool that allows for the detection of protein proteoforms in bottom-up proteomics data. Described in a paper published in June in Nature Communications, the tool, called COPF (COrrelation-based functional ProteoForm) uses peptide correlation analysis to detect differences in proteoform populations across different samples or conditions and could aid researchers as they seek to better understand the role different protein forms play in biology and disease. The human genome is thought to have around 20,000 protein coding genes, but many of these 20,000 proteins exist in the body in various forms, differentiated by, for instance, post-translational modifications or amino acid substitutions. These different forms are called proteoforms, and it is widely believed that biological processes are guided not just by the proteins present but by what proteoforms are present and in what proportions. Traditional bottom-up proteomics workflows have provided only limited insight into proteoform populations however, due to the fact that the presence of a particular protein is typically inferred by the detection of just a few of its peptides and that digesting proteins into peptides for mass spec analysis makes it near impossible to link a modified peptide back to a particular proteoform. Some proteomics researchers have addressed this issue by moving to top-down proteomics, which looks at intact proteins, allowing them to better distinguish between different proteoforms. Top-down proteomics is very technically challenging, however, and is not yet able to analyze proteins with the breadth and depth of bottom-up workflows. Recently, the development of more reproducible and higher-throughput bottom-up workflows, and particular workflows using data independent-acquisition (DIA) mass spectrometry, have allowed researchers like the Nature Communications authors to apply peptide correlation analysis to the study of proteoforms. Peptide correlation analysis looks at differences in peptide behavior within and across proteins in bottom-up data. Researchers have developed a number of approaches for turning peptide measurements into protein data, with most working under the assumption that peptides from the same protein will behave the same way. In practice, though, that isn't the case. On one hand, there are a number of technical reasons why two peptides from the same protein may not behave the same way. For instance, different digestion efficiencies could lead to some peptides being more abundant than others. Different ionization efficiencies could similarly make one peptide more likely than another to be detected by the mass spec. The presence of different proteoforms could also play a role. For instance, if a protein is present in both a full-length and truncated form, expression changes observed in the full-length form wouldn't be observable if the peptide being measured wasn't present in the truncated form. Not only would this throw off protein-level quantitation, but it would also mask relative changes in the two protein forms that could be biologically important. A major challenge to applying this insight has been determining which differences in peptide behavior reflect real technical or biological variation and which are just noise, noted Hannes R\u00f6st, research chair in mass spectrometry-based personalized medicine at the University of Toronto and an author on the Nature Communications study. \"In many cases [such variation] was noise,\" he said. \"When you look at traditional shotgun proteomics workflows and data analyses, really the power is not at the peptide-level quantification but at the protein level from the aggregation of multiple peptides. On the peptide level you see a lot of noise, and I think that has prevented us from using this observation that individual peptides could yield a lot of interested information because people really only looked at the protein-level data, because that is what they trusted.\" R\u00f6st said that the development of targeted protein quantitation approaches like multiple-reaction monitoring (MRM) has demonstrated that individual peptides can be measured with high accuracy, and the development of DIA mass spec approaches has enabled MRM-style peptide quantitation at the proteome scale. At the same time, improvements in mass spec technology have allowed researchers to collect the kind of large and reproducible datasets required for peptide correlation analysis, he said. \"These are types of experiments we wouldn't have imagined 10 years ago, because for correlation-based approaches to work, you need a relatively large number of samples, and you need low variance,\" he said. \"We are not detecting [proteoforms] that are not changing between different [conditions], we are only detecting those that change. And for this to work we need to have multiple replicates and we need to have different conditions and to be able to measure these peptides with high quantitative accuracy across these conditions.\" The COPF tool looks at the intensities of peptides coming from a particular protein across all the samples measured in an experiment and then calculates peptide correlations for all the pairs of peptides coming from that protein and uses hierarchical clustering to divide the peptides into two clusters. It then scores the likelihood that multiple proteoforms of a protein are present by comparing the level of peptide correlation between the clusters to the level of in-cluster variation. The tool does not identify the specific modifications or variations that distinguish the different proteoforms but rather the peptides that appear to differentiate between the forms of the protein in the different biological contexts investigated. Analyzing a DIA dataset that looked at five different tissue types across eight different mice, COPF identified 63 proteins that exhibited different proteoform groups, including proteins with known tissue-specific splice variants. The researchers also identified proteoforms created by proteolytic and autocatalytic cleavage and phosphorylation, indicating, they wrote, that the tool is \"agnostic to the different mechanisms by which proteoforms can be generated inside the cell.\" The development of COPF follows the publication last year of a study by researchers at Barts Cancer Institute and the University of Wisconsin-Madison detailing another peptide correlation analysis tool for identifying proteoforms in bottom-up data called PeCorA. Unlike COPF, which requires proteoforms to differ by two or more peptides, PeCorA can detect proteoforms based on single peptide differences. This makes it a potentially more sensitive tool but also less specific than COPF, R\u00f6st said. More generally, he said that he expected ongoing improvements in mass spec technology would further improve peptide correlation-based approaches like COPF and PeCorA by boosting peptide coverage. \"To kind of cover every possible protein isoform we would need to have complete coverage of every protein, and unfortunately we are currently quite far away from having peptide-level coverage of every protein,\" he said. \"I think that is currently one of the limitations where we are kind of hitting a wall.\" R\u00f6st added that his lab has begun acquiring data on Bruker's timsTOF Pro platform, \"and there we definitely see both an increase in protein coverage and also in the number of peptides we can measure.\" \"That's why I'm very optimistic that while this is just the first implementation of the method, the data we are producing at this moment is much more complete, and therefore I think it would be even more suitable to our approach than the data we used in the paper,\" he said.","title":"Notes"},{"location":"pilot/Notes/#notes","text":"","title":"Notes"},{"location":"pilot/Notes/#mz-chatgpt","text":"In the context of mass spectrometry, the term \"charge\" refers to the electrical charge of an ion. Mass spectrometry is a technique used to analyze the mass-to-charge ratio (m/z) of ions. Monoisotopic m/z refers to the mass-to-charge ratio of the monoisotopic peak, which is the peak corresponding to the ion containing the most abundant isotope of each element. The Max Isotope Time Centroid is the time at which the intensity-weighted average of the isotope distribution reaches its maximum value. This parameter is important for accurately determining the mass and charge state of an ion. The charge of an ion is a crucial piece of information in mass spectrometry. It is used in the calculation of the mass-to-charge ratio (m/z) of an ion, which is a key parameter in identifying and characterizing molecules. The m/z value is used to determine the mass of an ion, and the charge is needed to convert the raw mass spectrometry data (measured in daltons) into the mass-to-charge ratio. The formula for calculating m/z is: \\[ m/z=\\frac{\\mbox{Mass of the ion}}{\\mbox{Charge of the ion\u200b}} \\] The charge of an ion is essential for interpreting mass spectrometry data, particularly in determining the mass-to-charge ratio (m/z) of ions, which is fundamental for identifying and characterizing molecules in a sample. When the charge is 0 for monoisotopic m/z, it implies a neutral species, and the mass spectrometry analysis might be focusing on the measurement of neutral molecules or particles. In proteomics, researchers often analyze proteins and peptides in their neutral forms. Techniques like matrix-assisted laser desorption/ionization (MALDI) are commonly used for the analysis of intact proteins in their neutral state","title":"m/z (ChatGPT)"},{"location":"pilot/Notes/#peptide-and-protein-id-using-openms-tools","text":"https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-oms/tutorial.html#peptide-identification","title":"Peptide and Protein ID using OpenMS tools"},{"location":"pilot/Notes/#pogo","text":"Fast Mapping of Peptides to Genomic Coordinates for Proteogenomic Analyses, https://www.sanger.ac.uk/tool/pogo/ , GitHub, https://github.com/cschlaffner/PoGo . It uses transcript translations and reference gene annotations to identify the genomic loci of peptides and post-translational modifications. Multiple occurrences of peptides in the input data resulting in the same genomic loci will be collapsed as a single occurrence in the output. The input format is a tab delimited file with four columns with file extensions such as .pogo, .txt, and *.tsv. Column Column header Description 1 Sample Name of sample or experiment 2 Peptide Peptide sequence with PSI-MS nodification names in round brackets following the mpdified amino acid, e.g. PEPT(Phopsho)IDE for a phosphorylated threonine 3 PSMs Number of peptide-spectrum matches (PSMs) for the given peptide, including those redundantly identified (peptides can be \u201cseen\u201d more than once in a run) 4 Quant Quantitative value for the given peptide in the given sample An example is established as follows, wget -S ftp://ftp.sanger.ac.uk/pub/teams/17/software/PoGo/PoGo_Testprocedures.zip unzip PoGo_Testprocedures.zip cd PoGo_Testprocedures/Testfiles module load ceuadmin/PoGo for Peptides in Testfile_experimental Testfile_small do PoGo -fasta input/gencode.v25.pc_translations.fa -gtf input/gencode.v25.annotation.gtf -in input/${Peptides}.txt done Output files are also contained in the input/ directory. GENCODE annotation data are available from https://www.gencodegenes.org/human/ and https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/ . The Java GUI, https://github.com/cschlaffner/PoGoGUI , is run as follows, java -jar PoGoGUI-v1.0.0.jar which requires PoGo executable as well. The source is compiled with maven, https://maven.apache.org/ , e.g., module load maven-3.5.0-gcc-5.4.0-3sgaeze mvn install assuming that pom.xml is available, e.g., /usr/local/Cluster-Apps/ceuadmin/PoGo/1.0.0/PoGoGUI/PoGoGUI .","title":"PoGo"},{"location":"pilot/Notes/#proteoform-analysis","text":"","title":"Proteoform Analysis"},{"location":"pilot/Notes/#eth-zurich-u-toronto-team-develops-tool-for-bottom-up-proteomics-proteoform-analysis","text":"Jul 28, 2021 | Adam Bonislawski NEW YORK \u2013 A team led by researchers at ETH Zurich and the University of Toronto has developed a tool that allows for the detection of protein proteoforms in bottom-up proteomics data. Described in a paper published in June in Nature Communications, the tool, called COPF (COrrelation-based functional ProteoForm) uses peptide correlation analysis to detect differences in proteoform populations across different samples or conditions and could aid researchers as they seek to better understand the role different protein forms play in biology and disease. The human genome is thought to have around 20,000 protein coding genes, but many of these 20,000 proteins exist in the body in various forms, differentiated by, for instance, post-translational modifications or amino acid substitutions. These different forms are called proteoforms, and it is widely believed that biological processes are guided not just by the proteins present but by what proteoforms are present and in what proportions. Traditional bottom-up proteomics workflows have provided only limited insight into proteoform populations however, due to the fact that the presence of a particular protein is typically inferred by the detection of just a few of its peptides and that digesting proteins into peptides for mass spec analysis makes it near impossible to link a modified peptide back to a particular proteoform. Some proteomics researchers have addressed this issue by moving to top-down proteomics, which looks at intact proteins, allowing them to better distinguish between different proteoforms. Top-down proteomics is very technically challenging, however, and is not yet able to analyze proteins with the breadth and depth of bottom-up workflows. Recently, the development of more reproducible and higher-throughput bottom-up workflows, and particular workflows using data independent-acquisition (DIA) mass spectrometry, have allowed researchers like the Nature Communications authors to apply peptide correlation analysis to the study of proteoforms. Peptide correlation analysis looks at differences in peptide behavior within and across proteins in bottom-up data. Researchers have developed a number of approaches for turning peptide measurements into protein data, with most working under the assumption that peptides from the same protein will behave the same way. In practice, though, that isn't the case. On one hand, there are a number of technical reasons why two peptides from the same protein may not behave the same way. For instance, different digestion efficiencies could lead to some peptides being more abundant than others. Different ionization efficiencies could similarly make one peptide more likely than another to be detected by the mass spec. The presence of different proteoforms could also play a role. For instance, if a protein is present in both a full-length and truncated form, expression changes observed in the full-length form wouldn't be observable if the peptide being measured wasn't present in the truncated form. Not only would this throw off protein-level quantitation, but it would also mask relative changes in the two protein forms that could be biologically important. A major challenge to applying this insight has been determining which differences in peptide behavior reflect real technical or biological variation and which are just noise, noted Hannes R\u00f6st, research chair in mass spectrometry-based personalized medicine at the University of Toronto and an author on the Nature Communications study. \"In many cases [such variation] was noise,\" he said. \"When you look at traditional shotgun proteomics workflows and data analyses, really the power is not at the peptide-level quantification but at the protein level from the aggregation of multiple peptides. On the peptide level you see a lot of noise, and I think that has prevented us from using this observation that individual peptides could yield a lot of interested information because people really only looked at the protein-level data, because that is what they trusted.\" R\u00f6st said that the development of targeted protein quantitation approaches like multiple-reaction monitoring (MRM) has demonstrated that individual peptides can be measured with high accuracy, and the development of DIA mass spec approaches has enabled MRM-style peptide quantitation at the proteome scale. At the same time, improvements in mass spec technology have allowed researchers to collect the kind of large and reproducible datasets required for peptide correlation analysis, he said. \"These are types of experiments we wouldn't have imagined 10 years ago, because for correlation-based approaches to work, you need a relatively large number of samples, and you need low variance,\" he said. \"We are not detecting [proteoforms] that are not changing between different [conditions], we are only detecting those that change. And for this to work we need to have multiple replicates and we need to have different conditions and to be able to measure these peptides with high quantitative accuracy across these conditions.\" The COPF tool looks at the intensities of peptides coming from a particular protein across all the samples measured in an experiment and then calculates peptide correlations for all the pairs of peptides coming from that protein and uses hierarchical clustering to divide the peptides into two clusters. It then scores the likelihood that multiple proteoforms of a protein are present by comparing the level of peptide correlation between the clusters to the level of in-cluster variation. The tool does not identify the specific modifications or variations that distinguish the different proteoforms but rather the peptides that appear to differentiate between the forms of the protein in the different biological contexts investigated. Analyzing a DIA dataset that looked at five different tissue types across eight different mice, COPF identified 63 proteins that exhibited different proteoform groups, including proteins with known tissue-specific splice variants. The researchers also identified proteoforms created by proteolytic and autocatalytic cleavage and phosphorylation, indicating, they wrote, that the tool is \"agnostic to the different mechanisms by which proteoforms can be generated inside the cell.\" The development of COPF follows the publication last year of a study by researchers at Barts Cancer Institute and the University of Wisconsin-Madison detailing another peptide correlation analysis tool for identifying proteoforms in bottom-up data called PeCorA. Unlike COPF, which requires proteoforms to differ by two or more peptides, PeCorA can detect proteoforms based on single peptide differences. This makes it a potentially more sensitive tool but also less specific than COPF, R\u00f6st said. More generally, he said that he expected ongoing improvements in mass spec technology would further improve peptide correlation-based approaches like COPF and PeCorA by boosting peptide coverage. \"To kind of cover every possible protein isoform we would need to have complete coverage of every protein, and unfortunately we are currently quite far away from having peptide-level coverage of every protein,\" he said. \"I think that is currently one of the limitations where we are kind of hitting a wall.\" R\u00f6st added that his lab has begun acquiring data on Bruker's timsTOF Pro platform, \"and there we definitely see both an increase in protein coverage and also in the number of peptides we can measure.\" \"That's why I'm very optimistic that while this is just the first implementation of the method, the data we are producing at this moment is much more complete, and therefore I think it would be even more suitable to our approach than the data we used in the paper,\" he said.","title":"ETH Zurich, U Toronto Team Develops Tool for Bottom-Up Proteomics Proteoform Analysis"},{"location":"pilot/autoencoder/","text":"Autoencoder As shown at R-bloggers , autoencoder is better at reconstructing the original data set than PCA when k is small, where k corresponds to the number of principal components in PCA or bottleneck dimension in AE, however the error converges as k increases. For very large data sets this difference will be larger and means a smaller data set could be used for the same error as PCA. When dealing with big data this is an important property`. The local adoption is ae_test.Rmd which produces ae_test.html and ae_test.pdf . Additional work will be on variatinoal autoencoder (VAE) as indicated in the references below. REFERENCES Bludau I, Frank M, D\u00f6rig C. et al. Systematic detection of functional proteoform groups from bottom-up proteomic datasets. Nat Commun 12, 3810 (2021). https://doi.org/10.1038/s41467-021-24030-x . Hofert M, Prasad A, Zhu M (2019). Quasi-Monte Carlo for multivariate distributions viagenerative neural networks. https://arxiv.org/abs/1811.00683 , https://CRAN.R-project.org/package=gnn . Kingma DP, Welling M (2014). Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114 , https://keras.rstudio.com/articles/examples/variational_autoencoder.html . Trivadis SK (2017). Variational autoencoders for anomaly detection. https://rpubs.com/zkajdan/308801 . URLs https://github.com/diazale/gt-dimred , https://github.com/lmcinnes/umap ( https://umap-learn.readthedocs.io/en/latest/ ).","title":"Autoencoder"},{"location":"pilot/autoencoder/#autoencoder","text":"As shown at R-bloggers , autoencoder is better at reconstructing the original data set than PCA when k is small, where k corresponds to the number of principal components in PCA or bottleneck dimension in AE, however the error converges as k increases. For very large data sets this difference will be larger and means a smaller data set could be used for the same error as PCA. When dealing with big data this is an important property`. The local adoption is ae_test.Rmd which produces ae_test.html and ae_test.pdf . Additional work will be on variatinoal autoencoder (VAE) as indicated in the references below.","title":"Autoencoder"},{"location":"pilot/autoencoder/#references","text":"Bludau I, Frank M, D\u00f6rig C. et al. Systematic detection of functional proteoform groups from bottom-up proteomic datasets. Nat Commun 12, 3810 (2021). https://doi.org/10.1038/s41467-021-24030-x . Hofert M, Prasad A, Zhu M (2019). Quasi-Monte Carlo for multivariate distributions viagenerative neural networks. https://arxiv.org/abs/1811.00683 , https://CRAN.R-project.org/package=gnn . Kingma DP, Welling M (2014). Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114 , https://keras.rstudio.com/articles/examples/variational_autoencoder.html . Trivadis SK (2017). Variational autoencoders for anomaly detection. https://rpubs.com/zkajdan/308801 .","title":"REFERENCES"},{"location":"pilot/autoencoder/#urls","text":"https://github.com/diazale/gt-dimred , https://github.com/lmcinnes/umap ( https://umap-learn.readthedocs.io/en/latest/ ).","title":"URLs"},{"location":"pilot/gwas2/","text":"gwas2 This is a promising alternative showing through RCN3/FCGRN with gwas2.sh (all with prefix=utils/). graph TB; gwas2.sh --> gwas.do gwas2.sh --> gwas2.do where gwas.do ( caprion.dat also contains _invn data) and gwas2.do ( gwas2_invn.do for _invn data) are for the pilot and batch 2 data, respectively. See gwas2 repository for additional information.","title":"gwas2"},{"location":"pilot/gwas2/#gwas2","text":"This is a promising alternative showing through RCN3/FCGRN with gwas2.sh (all with prefix=utils/). graph TB; gwas2.sh --> gwas.do gwas2.sh --> gwas2.do where gwas.do ( caprion.dat also contains _invn data) and gwas2.do ( gwas2_invn.do for _invn data) are for the pilot and batch 2 data, respectively. See gwas2 repository for additional information.","title":"gwas2"}]}